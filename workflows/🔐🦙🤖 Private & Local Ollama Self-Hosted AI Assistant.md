# 🔐🦙🤖 Private & Local Ollama Self-Hosted AI Assistant

## Description
This workflow creates a private and local AI assistant using the Ollama model hosted internally. It processes chat messages and generates responses using the Llama model.

## Nodes Used
- **When chat message received**: Triggers the workflow when a chat message is received.
- **Basic LLM Chain**: Processes user prompts and generates responses.
- **Ollama Model**: Utilizes the Llama model for AI responses.

## Web Services Involved
- **Ollama**
